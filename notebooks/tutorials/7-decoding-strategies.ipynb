{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f35f43a-a799-4e5b-8c6b-51e08932b61b",
   "metadata": {},
   "source": [
    "# RL4CO Decoding Strategies Notebook\n",
    "\n",
    "This notebook demonstrates how to utilize the different decoding strategies available in rl4co/models/nn/dec_strategies.py during the different phases of model development. We will also demonstrate how to evaluate the model for different decoding strategies on the test dataset. \n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/ai4co/rl4co/blob/main/notebooks/tutorials/7-decoding-strategies.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866f4981-7358-4086-9044-6df6a4ba8fea",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7538da3e-67df-4c72-9acb-345a3bc9fba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment the following line to install the package from PyPI\n",
    "## You may need to restart the runtime in Colab after this\n",
    "## Remember to choose a GPU runtime for faster training!\n",
    "\n",
    "# !pip install rl4co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4380f62f-bde8-4fc5-aa1a-072d5be58a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from rl4co.envs import TSPEnv\n",
    "from rl4co.models.zoo import AttentionModel, AttentionModelPolicy\n",
    "from rl4co.utils.trainer import RL4COTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566e2f04-0576-4789-ac25-706ef3ebb7ca",
   "metadata": {},
   "source": [
    "### Setup Policy and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c9a2ac-a2cc-4a90-a810-75a092fa4890",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# RL4CO env based on TorchRL\n",
    "env = TSPEnv(num_loc=10) \n",
    "\n",
    "# Policy: neural network, in this case with encoder-decoder architecture\n",
    "policy = AttentionModelPolicy(env.name, \n",
    "                              embedding_dim=128,\n",
    "                              num_encoder_layers=3,\n",
    "                              num_heads=8,\n",
    "                            )\n",
    "\n",
    "# Model: default is AM with REINFORCE and greedy rollout baseline\n",
    "model = AttentionModel(env, \n",
    "                       baseline=\"rollout\",\n",
    "                       batch_size = 128,\n",
    "                       val_batch_size = 512,\n",
    "                       test_batch_size = 512,\n",
    "                       train_data_size=1_00,\n",
    "                       val_data_size=1_000,\n",
    "                       test_data_size=1_000,\n",
    "                       optimizer_kwargs={\"lr\": 1e-4},\n",
    "                       policy_kwargs={  # we can specify the decode types using the policy_kwargs\n",
    "                           \"train_decode_type\": \"sampling\",\n",
    "                           \"val_decode_type\": \"greedy\",\n",
    "                           \"test_decode_type\": \"beam_search\",\n",
    "                       }\n",
    "                       ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfa0c33-0f09-4316-84f9-25f6e8f8dfc0",
   "metadata": {},
   "source": [
    "### Setup Trainer and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e7840f-c3b7-4f47-b694-f00db7f25896",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RL4COTrainer(\n",
    "    max_epochs=2,\n",
    "    logger=None,\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e92694-7a6c-4b86-b1f7-7fade55fdef5",
   "metadata": {},
   "source": [
    "### Test the model using Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea794e13-5af6-41cc-b3cc-1a6b42520086",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# here we evaluate the model on the test set using the beam search decoding strategy as declared in the model constructor\n",
    "trainer.test(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ec98df-17d3-4250-9a9f-c1d510175934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can simply change the decoding type of the current model instance\n",
    "model.policy.test_decode_type = \"greedy\"\n",
    "trainer.test(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f2744e-83ea-402a-83ec-94cbf12a0870",
   "metadata": {},
   "source": [
    "### Manual Test Loop\n",
    "\n",
    "Let's compare beam search with a greedy decoding strategy by manually looping over our test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b89503-f416-4b73-a5dc-1f1dfd7369e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_rewards = []\n",
    "for batch in model.test_dataloader():\n",
    "    td = env.reset(batch)\n",
    "    with torch.no_grad():\n",
    "        # in a manual loop we can dynamically specify the decode type\n",
    "        out = model(td, decode_type=\"beam_search\", beam_width=10)\n",
    "    bs_rewards.append(out[\"reward\"])\n",
    "print(\"Average reward is %s\" % torch.cat(bs_rewards).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2103e579-d35f-4496-b401-47e9d3be7caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_rewards = []\n",
    "for batch in model.test_dataloader():\n",
    "    td = env.reset(batch)\n",
    "    with torch.no_grad():\n",
    "        out = model(td, decode_type=\"greedy\")\n",
    "    bs_rewards.append(out[\"reward\"])\n",
    "print(\"Average reward is %s\" % torch.cat(bs_rewards).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b902d8-446a-4d3e-b14e-673560ca7af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_rewards = []\n",
    "for batch in model.test_dataloader():\n",
    "    td = env.reset(batch)\n",
    "    bs = batch.batch_size[0]\n",
    "    with torch.no_grad():\n",
    "        out = model(td, decode_type=\"multistart_greedy\", num_starts=10, return_actions=True)\n",
    "        rewards = torch.stack(out[\"reward\"].split(bs), 1).max(1).values\n",
    "    bs_rewards.append(rewards)\n",
    "print(\"Average reward is %s\" % torch.cat(bs_rewards).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce439b05-9c60-42d0-b51b-07412b279e0a",
   "metadata": {},
   "source": [
    "We can see that beam search finds a better solution than the greedy decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e1793c-5bff-4a34-81f7-de080551ffad",
   "metadata": {},
   "source": [
    "### Digging deeper into beam search solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb7284a-6b3f-41fa-9589-d41dd7458721",
   "metadata": {},
   "source": [
    "We can also analyze the different solutions obtained via beam search when passing \"select_best=False\" to the forward pass of the policy. The solutions in this case are sorted per instance-wise, that is:\n",
    "\n",
    "- instance1_solution1\n",
    "- instance2_solution1\n",
    "- instance3_solution1\n",
    "- instance1_solution2\n",
    "- instance2_solution2\n",
    "- instance3_solution2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d476c7a-aa23-45bb-b3e9-441992dcdf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "td = env.reset(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e59a46-8aea-4739-a16d-913e3d3b8d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = batch.batch_size[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01727816-25ff-4a21-b092-b3a72be19343",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(td, decode_type=\"beam_search\", beam_width=5, select_best=False, return_actions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5366d787-2ac7-4d0c-bffc-926d64c82b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we split the sequence ofter every \"batch_size\" instances, then stack the different solutions obtained for each minibatch instance by the beam search together.\n",
    "actions_stacked = torch.stack(out[\"actions\"].split(bs), 1)\n",
    "rewards_stacked = torch.stack(out[\"reward\"].split(bs), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee02330-965b-48f3-8cf8-ed20ed7e1af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "batch_instance = 0\n",
    "for i, actions in enumerate(actions_stacked[batch_instance].cpu()):\n",
    "    reward = rewards_stacked[batch_instance, i]\n",
    "    _, ax = plt.subplots()\n",
    "    \n",
    "    env.render(td[0], actions, ax=ax)\n",
    "    ax.set_title(\"Reward: %s\" % reward.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
