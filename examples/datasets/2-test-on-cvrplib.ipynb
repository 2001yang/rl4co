{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model on VRPLib\n",
    "\n",
    "In this notebook, we will test the trained model's performance on the VRPLib benchmark. We will use the trained model from the previous notebook.\n",
    "\n",
    "[VRPLIB](http://vrp.galgos.inf.puc-rio.br/index.php/en/) is a collection of instances related to the CVRP, which is a classic optimization challenge in the field of logistics and transportation. \n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/ai4co/rl4co/blob/main/examples/datasets/2-test-on-cvrplib.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
    "\n",
    "## Before we start\n",
    "\n",
    "To use the VRPLib, we strongly recomment to use the Python `vrplib` tool:\n",
    "\n",
    "[VRPLib](https://github.com/leonlan/VRPLIB) is a Python package for working with Vehicle Routing Problem (VRP) instances. This tool can help us easily load the VRPLib instances and visualize the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Uncomment the following line to install the package from PyPI. Remember to choose a GPU runtime for faster training!\n",
    "\n",
    "> Note: You may need to restart the runtime in Colab after this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# !pip install rl4co[graph] # include torch-geometric\n",
    "\n",
    "## NOTE: to install latest version from Github (may be unstable) install from source instead:\n",
    "# !pip install git+https://github.com/ai4co/rl4co.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the `vrplib` package\n",
    "# !pip install vrplib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import vrplib\n",
    "\n",
    "from rl4co.envs import TSPEnv, CVRPEnv\n",
    "from rl4co.models.zoo.am import AttentionModel, AttentionModelPolicy\n",
    "from rl4co.utils.trainer import RL4COTrainer\n",
    "from rl4co.utils.decoding import get_log_likelihood\n",
    "from rl4co.models.zoo import EAS, EASLay, EASEmb, ActiveSearch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from math import ceil\n",
    "from einops import repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from checkpoint; alternatively, simply instantiate a new model\n",
    "# Note the model is trained for CVRP problem\n",
    "checkpoint_path = \"../cvrp-20.ckpt\" # modify the path to your checkpoint file\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load checkpoint\n",
    "# checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# lit_model = AttentionModel.load_from_checkpoint(checkpoint_path, load_baseline=False)\n",
    "\n",
    "env = CVRPEnv()\n",
    "policy = AttentionModelPolicy(env_name=env.name)\n",
    "\n",
    "# policy, env = lit_model.policy, lit_model.env\n",
    "policy = policy.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download vrp problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/botu/mambaforge/envs/rl4co/lib/python3.11/site-packages/vrplib/download/list_names.py:32: DeprecationWarning: The function 'list_names' is deprecated and will be removed in the next major version (vrplib v2.0.0).\n",
      "  warnings.warn(msg, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "problem_names = vrplib.list_names(low=50, high=200, vrp_type='cvrp') \n",
    "\n",
    "instances = [] # Collect Set A, B, E, F, M datasets\n",
    "for name in problem_names:\n",
    "    if 'A' in name:\n",
    "        instances.append(name)\n",
    "    elif 'B' in name:\n",
    "        instances.append(name)\n",
    "    elif 'E' in name:\n",
    "        instances.append(name)\n",
    "    elif 'F' in name:\n",
    "        instances.append(name)\n",
    "    elif 'M' in name and 'CMT' not in name:\n",
    "        instances.append(name)\n",
    "\n",
    "# Modify the path you want to save \n",
    "# Note: we don't have to create this folder in advance\n",
    "path_to_save = './vrplib/' \n",
    "\n",
    "try:\n",
    "    os.makedirs(path_to_save)\n",
    "    for instance in tqdm(instances):\n",
    "        vrplib.download_instance(instance, path_to_save)\n",
    "        vrplib.download_solution(instance, path_to_save)\n",
    "except: # already exist\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils function\n",
    "def normalize_coord(coord:torch.Tensor) -> torch.Tensor:\n",
    "    x, y = coord[:, 0], coord[:, 1]\n",
    "    x_min, x_max = x.min(), x.max()\n",
    "    y_min, y_max = y.min(), y.max()\n",
    "    \n",
    "    x_scaled = (x - x_min) / (x_max - x_min) \n",
    "    y_scaled = (y - y_min) / (y_max - y_min)\n",
    "    coord_scaled = torch.stack([x_scaled, y_scaled], dim=1)\n",
    "    return coord_scaled "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensordict import TensorDict\n",
    "\n",
    "problem = vrplib.read_instance(os.path.join(path_to_save, instance+'.vrp'))\n",
    "\n",
    "# problem\n",
    "coords = torch.tensor(problem['node_coord']).float()\n",
    "coords_norm = normalize_coord(coords)\n",
    "demand = torch.tensor(problem['demand'][1:]).float()\n",
    "capacity = problem['capacity']\n",
    "n = coords.shape[0]\n",
    "td = TensorDict({\n",
    "    'depot': coords_norm[0,:],\n",
    "    'locs': coords_norm[1:,:],\n",
    "    'demand': demand / capacity, # normalized demand\n",
    "    'capacity': capacity, # original capacity, not needed for inference\n",
    "})\n",
    "td = td[None] # add batch dimension, in this case just 1\n",
    "\n",
    "# start\n",
    "td_reset =  env.reset(td.to(device)).to(device)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = policy(td_reset, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 61])\n",
      "Problem: A-n53-k7        Cost: 2635       Optimal Cost: 1010      \t Gap: 160.89%\n",
      "torch.Size([1, 61])\n",
      "Problem: A-n54-k7        Cost: 2966       Optimal Cost: 1167      \t Gap: 154.16%\n",
      "torch.Size([1, 65])\n",
      "Problem: A-n55-k9        Cost: 2793       Optimal Cost: 1073      \t Gap: 160.30%\n",
      "torch.Size([1, 74])\n",
      "Problem: A-n60-k9        Cost: 3225       Optimal Cost: 1354      \t Gap: 138.18%\n",
      "torch.Size([1, 70])\n",
      "Problem: A-n61-k9        Cost: 2799       Optimal Cost: 1034      \t Gap: 170.70%\n",
      "torch.Size([1, 71])\n",
      "Problem: A-n62-k8        Cost: 3412       Optimal Cost: 1288      \t Gap: 164.91%\n",
      "torch.Size([1, 76])\n",
      "Problem: A-n63-k9        Cost: 3735       Optimal Cost: 1616      \t Gap: 131.13%\n",
      "torch.Size([1, 73])\n",
      "Problem: A-n63-k10       Cost: 3143       Optimal Cost: 1314      \t Gap: 139.19%\n",
      "torch.Size([1, 72])\n",
      "Problem: A-n64-k9        Cost: 3391       Optimal Cost: 1401      \t Gap: 142.04%\n",
      "torch.Size([1, 75])\n",
      "Problem: A-n65-k9        Cost: 3364       Optimal Cost: 1174      \t Gap: 186.54%\n",
      "torch.Size([1, 77])\n",
      "Problem: A-n69-k9        Cost: 3482       Optimal Cost: 1159      \t Gap: 200.43%\n",
      "torch.Size([1, 94])\n",
      "Problem: A-n80-k10       Cost: 4766       Optimal Cost: 1763      \t Gap: 170.33%\n",
      "torch.Size([1, 58])\n",
      "Problem: B-n51-k7        Cost: 3423       Optimal Cost: 1032      \t Gap: 231.69%\n",
      "torch.Size([1, 59])\n",
      "Problem: B-n52-k7        Cost: 2502       Optimal Cost: 747       \t Gap: 234.94%\n",
      "torch.Size([1, 62])\n",
      "Problem: B-n56-k7        Cost: 2843       Optimal Cost: 707       \t Gap: 302.12%\n",
      "torch.Size([1, 72])\n",
      "Problem: B-n57-k7        Cost: 3242       Optimal Cost: 1153      \t Gap: 181.18%\n",
      "torch.Size([1, 66])\n",
      "Problem: B-n57-k9        Cost: 3234       Optimal Cost: 1598      \t Gap: 102.38%\n",
      "torch.Size([1, 77])\n",
      "Problem: B-n63-k10       Cost: 3789       Optimal Cost: 1496      \t Gap: 153.28%\n",
      "torch.Size([1, 73])\n",
      "Problem: B-n64-k9        Cost: 2823       Optimal Cost: 861       \t Gap: 227.87%\n",
      "torch.Size([1, 82])\n",
      "Problem: B-n66-k9        Cost: 3137       Optimal Cost: 1316      \t Gap: 138.37%\n",
      "torch.Size([1, 83])\n",
      "Problem: B-n67-k10       Cost: 3217       Optimal Cost: 1032      \t Gap: 211.72%\n",
      "torch.Size([1, 76])\n",
      "Problem: B-n68-k9        Cost: 3672       Optimal Cost: 1272      \t Gap: 188.68%\n",
      "torch.Size([1, 88])\n",
      "Problem: B-n78-k10       Cost: 4151       Optimal Cost: 1221      \t Gap: 239.97%\n",
      "torch.Size([1, 56])\n",
      "Problem: E-n51-k5        Cost: 1595       Optimal Cost: 521       \t Gap: 206.14%\n",
      "torch.Size([1, 84])\n",
      "Problem: E-n76-k7        Cost: 2457       Optimal Cost: 682       \t Gap: 260.26%\n",
      "torch.Size([1, 84])\n",
      "Problem: E-n76-k8        Cost: 2459       Optimal Cost: 735       \t Gap: 234.56%\n",
      "torch.Size([1, 87])\n",
      "Problem: E-n76-k10       Cost: 2479       Optimal Cost: 830       \t Gap: 198.67%\n",
      "torch.Size([1, 90])\n",
      "Problem: E-n76-k14       Cost: 2572       Optimal Cost: 1021      \t Gap: 151.91%\n",
      "torch.Size([1, 109])\n",
      "Problem: E-n101-k8       Cost: 3190       Optimal Cost: 815       \t Gap: 291.41%\n",
      "torch.Size([1, 115])\n",
      "Problem: E-n101-k14      Cost: 3281       Optimal Cost: 1067      \t Gap: 207.50%\n",
      "torch.Size([1, 76])\n",
      "Problem: F-n72-k4        Cost: 1147       Optimal Cost: 237       \t Gap: 383.97%\n",
      "torch.Size([1, 143])\n",
      "Problem: F-n135-k7       Cost: 6358       Optimal Cost: 1162      \t Gap: 447.16%\n",
      "torch.Size([1, 114])\n",
      "Problem: M-n101-k10      Cost: 3891       Optimal Cost: 820       \t Gap: 374.51%\n",
      "torch.Size([1, 133])\n",
      "Problem: M-n121-k7       Cost: 6241       Optimal Cost: 1034      \t Gap: 503.58%\n",
      "torch.Size([1, 162])\n",
      "Problem: M-n151-k12      Cost: 5003       Optimal Cost: 1015      \t Gap: 392.91%\n",
      "torch.Size([1, 218])\n",
      "Problem: M-n200-k16      Cost: 6730       Optimal Cost: 1274      \t Gap: 428.26%\n",
      "torch.Size([1, 217])\n",
      "Problem: M-n200-k17      Cost: 6771       Optimal Cost: 1275      \t Gap: 431.06%\n"
     ]
    }
   ],
   "source": [
    "# Import augmented utils\n",
    "from rl4co.data.transforms import (\n",
    "    StateAugmentation as SymmetricStateAugmentation)\n",
    "from rl4co.utils.ops import batchify, unbatchify\n",
    "\n",
    "num_augment = 100\n",
    "augmentation = SymmetricStateAugmentation(num_augment=num_augment)\n",
    "\n",
    "for instance in instances:\n",
    "    problem = vrplib.read_instance(os.path.join(path_to_save, instance+'.vrp'))\n",
    "\n",
    "    # problem\n",
    "    coords = torch.tensor(problem['node_coord']).float()\n",
    "    coords_norm = normalize_coord(coords)\n",
    "    demand = torch.tensor(problem['demand'][1:]).float()\n",
    "    capacity = problem['capacity']\n",
    "    n = coords.shape[0]\n",
    "    td = TensorDict({\n",
    "        'depot': coords_norm[0,:],\n",
    "        'locs': coords_norm[1:,:],\n",
    "        'demand': demand / capacity, # normalized demand\n",
    "        'capacity': capacity, # original capacity, not needed for inference\n",
    "    })\n",
    "    td = td[None] # add batch dimension, in this case just 1\n",
    "    td =  env.reset(td.to(device))   \n",
    "    \n",
    "    # Get the solution from the policy\n",
    "    with torch.inference_mode():\n",
    "        out = policy(\n",
    "            td.clone(), env, decode_type='sampling', num_starts=128, select_best=True, multisample=True\n",
    "        )\n",
    "\n",
    "    # reward = env.get_reward(td, out['actions'])\n",
    "    # reward = unbatchify(reward, num_augment)\n",
    "    \n",
    "    # set back to original coordinates\n",
    "    td[\"locs\"] = coords[None] # note: these contain already the depot\n",
    "    print(out[\"actions\"].shape)\n",
    "    reward = env.get_reward(td, out['actions'])\n",
    "    cost = ceil(-1 * torch.max(reward).item())\n",
    "\n",
    "    # Load the optimal cost\n",
    "    solution = vrplib.read_solution(os.path.join(path_to_save, instance+'.sol'))\n",
    "    optimal_cost = solution['cost']\n",
    "\n",
    "    # Calculate the gap and print\n",
    "    gap = (cost - optimal_cost) / optimal_cost\n",
    "    print(f'Problem: {instance:<15} Cost: {cost:<10} Optimal Cost: {optimal_cost:<10}\\t Gap: {gap:.2%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl4co",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
